{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-28T13:32:25.712229Z",
     "iopub.status.busy": "2025-07-28T13:32:25.711667Z",
     "iopub.status.idle": "2025-07-28T13:32:39.165900Z",
     "shell.execute_reply": "2025-07-28T13:32:39.165099Z",
     "shell.execute_reply.started": "2025-07-28T13:32:25.712205Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 13:32:27.505161: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753709547.684618      96 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753709547.739450      96 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras.layers import Layer, Input, Dense, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T13:32:39.167930Z",
     "iopub.status.busy": "2025-07-28T13:32:39.166988Z",
     "iopub.status.idle": "2025-07-28T13:32:41.007209Z",
     "shell.execute_reply": "2025-07-28T13:32:41.005994Z",
     "shell.execute_reply.started": "2025-07-28T13:32:39.167908Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip freeze > MLP_FFM_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn==1.4.2 imbalanced-learn==0.12.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --force-reinstall --no-cache-dir scikit-learn==1.4.2 imbalanced-learn==0.12.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.26.4 --force-reinstall --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFM_Layer(Layer):\n",
    "    def __init__(self, sparse_feature_columns, k, w_reg=1e-6, v_reg=1e-6):\n",
    "        super(FFM_Layer, self).__init__()\n",
    "        self.sparse_feature_columns = sparse_feature_columns\n",
    "        self.k = k\n",
    "        self.w_reg = w_reg\n",
    "        self.v_reg = v_reg\n",
    "        self.index_offset = []\n",
    "        self.total_feat = 0\n",
    "        for feat in self.sparse_feature_columns:\n",
    "            self.index_offset.append(self.total_feat)\n",
    "            self.total_feat += feat['feat_num']\n",
    "        self.field_num = len(self.sparse_feature_columns)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w0 = self.add_weight(name='w0', shape=(1,), initializer='zeros', trainable=True)\n",
    "        self.w = self.add_weight(name='w', shape=(self.total_feat, 1),\n",
    "                                 initializer='random_normal',\n",
    "                                 regularizer=regularizers.l2(self.w_reg), trainable=True)\n",
    "        self.v = self.add_weight(name='v',\n",
    "                                 shape=(self.total_feat, self.field_num, self.k),\n",
    "                                 initializer='random_normal',\n",
    "                                 regularizer=regularizers.l2(self.v_reg), trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = inputs + tf.constant(self.index_offset, dtype=tf.int32)\n",
    "        first_order = self.w0 + tf.reduce_sum(tf.nn.embedding_lookup(self.w, inputs), axis=1)\n",
    "        second_order = 0\n",
    "        latent = tf.nn.embedding_lookup(self.v, inputs)\n",
    "        latent_sum = tf.reduce_sum(latent, axis=2)\n",
    "        for i in range(self.field_num):\n",
    "            for j in range(i + 1, self.field_num):\n",
    "                vi = latent_sum[:, i, :]\n",
    "                vj = latent_sum[:, j, :]\n",
    "                second_order += tf.reduce_sum(vi * vj, axis=1, keepdims=True)\n",
    "        return first_order + second_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ffm_with_mlp_numerical(sparse_cols, num_numerical, mlp_units=[64, 32]):\n",
    "    k = 8\n",
    "    sparse_input = Input(shape=(len(sparse_cols),), dtype=tf.int32, name='cat_input')\n",
    "    ffm_output = FFM_Layer(sparse_cols, k, w_reg=1e-6, v_reg=1e-6)(sparse_input)\n",
    "\n",
    "    num_input = Input(shape=(num_numerical,), dtype=tf.float32, name='num_input')\n",
    "    x = num_input\n",
    "    for idx, units in enumerate(mlp_units):\n",
    "        x = Dense(units, activation='relu', name=f'num_dense_{idx}')(x)\n",
    "    mlp_output = Dense(1, name='num_out')(x)\n",
    "\n",
    "    logit = Add(name='logit')([ffm_output, mlp_output])\n",
    "    proba = tf.keras.activations.sigmoid(logit)\n",
    "\n",
    "    model = Model(inputs=[sparse_input, num_input], outputs=proba)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['AUC'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ros(X,y,ratio):\n",
    "    ros = RandomOverSampler(sampling_strategy=ratio,random_state=42)\n",
    "    X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_train(csv_path, label_col='y'):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    X_resampled, y_resampled = apply_ros(df.drop(columns=[label_col]),df[label_col],0.1)\n",
    "\n",
    "    extra = X_resampled[['id1','id2','id5','id3']]\n",
    "    X = X_resampled.drop(columns=['id1','id2','id5','id3'])\n",
    "    y = y_resampled.values\n",
    "    \n",
    "    label = pd.read_csv('/kaggle/input/prepared-data-mlp-ffm/actual_final_label.csv')\n",
    "    \n",
    "    cat_cols = label[label['Type']=='Categorical']['masked_column'].to_list()\n",
    "    cat_cols.extend(label[label['Type']=='One hot encoded']['masked_column'].to_list())\n",
    "    cat_cols.remove('id3')\n",
    "    num_cols = label[label['Type']=='Numerical']['masked_column'].to_list()\n",
    "    num_cols.remove('id5')\n",
    "\n",
    "    X[cat_cols] = X[cat_cols].astype('category')\n",
    "\n",
    "    # Encode categorical columns\n",
    "    sparse_input_data = []\n",
    "    sparse_cols_metadata = []\n",
    "\n",
    "    encoder = {}\n",
    "    for col in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        num_classes = X[col].nunique()\n",
    "        sparse_input_data.append(X[col].values)\n",
    "        sparse_cols_metadata.append({'feat': col, 'feat_num': num_classes})\n",
    "        encoder[col] = le\n",
    "\n",
    "    sparse_input = np.stack(sparse_input_data, axis=1)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    numerical_input = scaler.fit_transform(X[num_cols].values.astype(np.float32))\n",
    "\n",
    "    return sparse_input, numerical_input, y, sparse_cols_metadata, len(num_cols), encoder, scaler, extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_test(csv_path, encoder=None, scaler=None):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    extra = df[['id1','id2','id3','id5']]\n",
    "    df = df.drop(columns=['id1','id2','id5','id3'])\n",
    "\n",
    "    # Separate label\n",
    "    X = df.copy()\n",
    "    \n",
    "    label = pd.read_csv('/kaggle/input/prepared-data-mlp-ffm/actual_final_label.csv')\n",
    "    \n",
    "    cat_cols = label[label['Type']=='Categorical']['masked_column'].to_list()\n",
    "    cat_cols.extend(label[label['Type']=='One hot encoded']['masked_column'].to_list())\n",
    "    cat_cols.remove('id3')\n",
    "    num_cols = label[label['Type']=='Numerical']['masked_column'].to_list()\n",
    "    num_cols.remove('id5')\n",
    "\n",
    "    X[cat_cols] = X[cat_cols].astype('category')\n",
    "\n",
    "    # Encode categorical columns\n",
    "    sparse_input_data = []\n",
    "    sparse_cols_metadata = []\n",
    "\n",
    "    for col in cat_cols:\n",
    "        le = encoder[col]\n",
    "        known_classes = set(le.classes_)\n",
    "        X[col] = X[col].astype(str).apply(lambda x: x if x in known_classes else 'unknown')\n",
    "        le.classes_ = np.append(le.classes_, 'unknown')\n",
    "        X[col] = le.transform(X[col])\n",
    "        num_classes = X[col].nunique()\n",
    "        sparse_input_data.append(X[col].values)\n",
    "        sparse_cols_metadata.append({'feat': col, 'feat_num': num_classes})\n",
    "\n",
    "    sparse_input = np.stack(sparse_input_data, axis=1)\n",
    "    \n",
    "    numerical_input = scaler.transform(X[num_cols].values.astype(np.float32))\n",
    "\n",
    "    return sparse_input, numerical_input, sparse_cols_metadata, len(num_cols), encoder, extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=7):\n",
    "    \"\"\"\n",
    "    actual: list of relevant ids (id3s with y==1)\n",
    "    predicted: list of predicted ids in rank order\n",
    "    \"\"\"\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    \n",
    "    return score / min(len(actual), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapk(df, k=7):\n",
    "    ap_scores = []\n",
    "\n",
    "    for _, group in df.groupby(\"id2\"):\n",
    "        actual = group[group[\"actual\"] == 1][\"id3\"].tolist()\n",
    "        predicted = group.sort_values(\"pred_proba\", ascending=False)[\"id3\"].tolist()\n",
    "        \n",
    "\n",
    "        if actual:\n",
    "            ap = apk(actual, predicted, k)\n",
    "            ap_scores.append(ap)\n",
    "\n",
    "    return np.mean(ap_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = '/kaggle/input/prepared-data-mlp-ffm/train_done.csv'  # Replace with your actual path\n",
    "\n",
    "X_cat, X_num, y, sparse_col_info, num_num, encoder, scaler, extra = prepare_data_train(csv_file_path)\n",
    "\n",
    "row_indices = np.arange(len(y))\n",
    "\n",
    "# Split\n",
    "X_cat_train, X_cat_val, X_num_train, X_num_val, y_train, y_val, idx_train, idx_val = train_test_split(\n",
    "    X_cat, X_num, y, row_indices, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Build model\n",
    "model = build_ffm_with_mlp_numerical(sparse_col_info, num_numerical=num_num, mlp_units=[128, 64])\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_AUC',\n",
    "    patience=2,\n",
    "    restore_best_weights=True,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "# Train\n",
    "model.fit([X_cat_train, X_num_train], y_train,\n",
    "          validation_data=([X_cat_val, X_num_val], y_val),\n",
    "          batch_size=1024, epochs=10,\n",
    "         callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([X_cat_val,X_num_val])\n",
    "extra = extra.iloc[idx_val].reset_index(drop=True).copy()\n",
    "\n",
    "extra['pred_proba'] = y_pred.ravel()\n",
    "extra['actual'] = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map7score = mapk(extra,k=7)\n",
    "print(map7score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_path = '/kaggle/input/prepared-data-mlp-ffm/test_done.csv'\n",
    "X_cat, X_num, sparse_col_info, num_num, encoder, extra = prepare_data_test(test_file_path,encoder=encoder,scaler=scaler)\n",
    "y_pred = model.predict([X_cat,X_num])\n",
    "extra['pred_proba'] = y_pred\n",
    "extra.sort_values(by=['id2', 'pred_proba'], ascending=[True, False], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra['pred']=1\n",
    "extra.reset_index(drop=True,inplace=True)\n",
    "extra.drop(columns=['pred_proba'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra.to_csv('submission3.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:light"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7839013,
     "sourceId": 12428865,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
