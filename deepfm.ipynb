{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn==1.4.2 imbalanced-learn==0.12.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --force-reinstall --no-cache-dir scikit-learn==1.4.2 imbalanced-learn==0.12.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.26.4 --force-reinstall --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-28T13:29:00.916599Z",
     "iopub.status.busy": "2025-07-28T13:29:00.916191Z",
     "iopub.status.idle": "2025-07-28T13:29:19.229227Z",
     "shell.execute_reply": "2025-07-28T13:29:19.228382Z",
     "shell.execute_reply.started": "2025-07-28T13:29:00.916561Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 13:29:03.252327: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753709343.510773      96 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753709343.582137      96 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Reshape, Add, Concatenate, Dropout, Activation, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T13:29:19.231521Z",
     "iopub.status.busy": "2025-07-28T13:29:19.230530Z",
     "iopub.status.idle": "2025-07-28T13:29:21.439936Z",
     "shell.execute_reply": "2025-07-28T13:29:21.438539Z",
     "shell.execute_reply.started": "2025-07-28T13:29:19.231497Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip freeze > deepfm_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeepFM(num_numeric, categorical_feature_info ):\n",
    "    # Define inputs: one for all numeric features, and one per categorical feature\n",
    "    numeric_input = Input(shape=(num_numeric,), name='numeric_input')\n",
    "    cat_inputs = []\n",
    "    for name, vocab_size in categorical_feature_info.items():\n",
    "        cat_inputs.append(Input(shape=(1,), name=f'{name}_input'))\n",
    "\n",
    "    # 1) First-order linear terms\n",
    "    linear_terms = []\n",
    "    # Numeric linear term: Dense(1) on numeric inputs\n",
    "    linear_terms.append(Dense(1, name='linear_numeric')(numeric_input))\n",
    "    # Categorical linear term: embedding of size 1 per category\n",
    "    for inp, (name, vocab_size) in zip(cat_inputs, categorical_feature_info.items()):\n",
    "        lin_emb = Embedding(input_dim=vocab_size, output_dim=1, name=f'linear_emb_{name}')(inp)\n",
    "        linear_terms.append(Flatten()(lin_emb))\n",
    "    linear_logit = Add(name='linear_logit')(linear_terms)\n",
    "\n",
    "    # 2) Second-order FM terms\n",
    "    embed_dim = 10 # Tune this as required\n",
    "    embeddings = []\n",
    "    for inp, (name, vocab_size) in zip(cat_inputs, categorical_feature_info.items()):\n",
    "        emb = Embedding(input_dim=vocab_size, output_dim=embed_dim, name=f'emb_{name}')(inp)\n",
    "        embeddings.append(Reshape((1, embed_dim))(emb))\n",
    "\n",
    "    # Stack to shape (None, num_cat, embed_dim)\n",
    "    concat_embeds = Concatenate(axis=1)(embeddings)  # shape (batch_size, p, k)\n",
    "    # Sum of embeddings: shape (batch_size, k)\n",
    "    sum_of_embeds = Lambda(lambda x: tf.reduce_sum(x, axis=1))(concat_embeds)\n",
    "    # Square of sum\n",
    "    square_of_sum = Lambda(lambda x: tf.square(x))(sum_of_embeds)\n",
    "    # Sum of squares\n",
    "    sum_of_squares = Lambda(lambda x: tf.reduce_sum(tf.square(x), axis=1))(concat_embeds)\n",
    "    # FM second-order vector (batch_size, k)\n",
    "    fm_vec = Lambda(lambda x: 0.5 * (x[0] - x[1]))([square_of_sum, sum_of_squares])\n",
    "    # Sum over latent dim to get scalar logit per sample\n",
    "    fm_logit = Lambda(lambda x: tf.reduce_sum(x, axis=1, keepdims=True), name='fm_logit')(fm_vec)\n",
    "\n",
    "    # 3) Deep part (DNN)\n",
    "    # Flatten embeddings for DNN input\n",
    "    flat_embeds = [Flatten()(emb) for emb in embeddings]  # each (batch_size, k)\n",
    "    dnn_input = Concatenate()(flat_embeds + [numeric_input])  # (batch_size, p*k + num_numeric)\n",
    "    # Pass through DNN layers\n",
    "    x = Dropout(0.3)(dnn_input)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    dnn_out = Dense(1, name='dnn_logit')(x)  # final logit from DNN\n",
    "\n",
    "    # Combine FM and DNN parts\n",
    "    final_logit = Add(name='final_logit')([linear_logit, fm_logit, dnn_out])\n",
    "    output = Activation('sigmoid')(final_logit)\n",
    "    \n",
    "    # Build and compile model\n",
    "    model = Model(inputs=[numeric_input] + cat_inputs, outputs=output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_constants():\n",
    "    train = pd.read_csv('/kaggle/input/prepared-data-mlp-ffm/train_done.csv')\n",
    "    dropped = []\n",
    "    for col in train.columns:\n",
    "        if train[col].nunique()<2:\n",
    "            dropped.append(col)\n",
    "    train.drop(columns=dropped,inplace=True)\n",
    "    label = pd.read_csv('/kaggle/input/prepared-data-mlp-ffm/actual_final_label.csv')\n",
    "    label = label[label['masked_column'].isin(train.columns)]\n",
    "    return train, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train(train, label):\n",
    "    # the model takes a list of [X_numeric, X_cat1, X_cat2, ...] as input\n",
    "    y = train['y']\n",
    "    X = []\n",
    "    cat_cols = []\n",
    "    encoder = {}\n",
    "    cat_cols.extend(label[label['Type']=='Categorical']['masked_column'].to_list())\n",
    "    cat_cols.extend(label[label['Type']=='One hot encoded']['masked_column'].to_list())\n",
    "    for col in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        train[col] = le.fit_transform(train[col].astype(str))\n",
    "        encoder[col] = le\n",
    "    num_cols = label[label['Type']=='Numerical']['masked_column'].to_list()\n",
    "    scaler = StandardScaler()\n",
    "    numerical_input = scaler.fit_transform(train[num_cols].values.astype(np.float32))\n",
    "    X.append(np.array(numerical_input))\n",
    "    for col in cat_cols:\n",
    "        X.append(np.array(train[col], dtype=np.int32).reshape(-1,1))\n",
    "    return X, y, encoder, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(train,label):\n",
    "    num_numeric = len(label[label['Type']=='Numerical']['masked_column'].to_list())\n",
    "    cat_feat_info = {}\n",
    "    cat_cols = []\n",
    "    cat_cols.extend(label[label['Type']=='Categorical']['masked_column'].to_list())\n",
    "    cat_cols.extend(label[label['Type']=='One hot encoded']['masked_column'].to_list())\n",
    "    cat_cols.remove('id3')\n",
    "    for col in cat_cols:\n",
    "        cat_feat_info[col] = train[col].nunique()\n",
    "    return num_numeric, cat_feat_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ros(X,y,ratio):\n",
    "    ros = RandomOverSampler(sampling_strategy=ratio,random_state=42)\n",
    "    X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "    X_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "    X_resampled['y'] = y_resampled\n",
    "    return X_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, label = remove_constants()\n",
    "train.drop(columns=['id1','id2','id5','id3'],inplace=True) # not training the model on offer id as well\n",
    "label = label[label['masked_column'].isin(train.columns)]\n",
    "train = apply_ros(train.drop(columns=['y']).copy(), train['y'], 0.2)\n",
    "train = train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "num_numeric, cat_feat_info = get_info(train,label)\n",
    "X, y, encoder, scaler = prepare_train(train, label)\n",
    "col_order = train.columns.to_list() # test data and train data should have the same feature order\n",
    "col_order.remove('y')\n",
    "del train\n",
    "del label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepFM(num_numeric, cat_feat_info)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(patience=2, restore_best_weights=True)\n",
    "model.fit(X, y, batch_size=1024, epochs=5, validation_split=0.1, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test(test, encoder, scaler):\n",
    "    label = pd.read_csv('/kaggle/input/prepared-data-mlp-ffm/actual_final_label.csv')\n",
    "    label = label[label['masked_column'].isin(test.columns)]\n",
    "    X = []\n",
    "    cat_cols = []\n",
    "    \n",
    "    cat_cols.extend(label[label['Type']=='Categorical']['masked_column'].to_list())\n",
    "    cat_cols.extend(label[label['Type']=='One hot encoded']['masked_column'].to_list())\n",
    "    for col in cat_cols:\n",
    "        le = encoder[col]\n",
    "        known = set(le.classes_)\n",
    "        test[col] = test[col].astype(str).apply(lambda x: x if x in known else 'unknown')\n",
    "        le.classes_ = np.append(le.classes_,'unknown')\n",
    "        test[col] = le.transform(test[col])\n",
    "    num_cols = label[label['Type']=='Numerical']['masked_column'].to_list()\n",
    "    numerical_input = scaler.transform(test[num_cols].values.astype(np.float32))\n",
    "    X.append(np.array(numerical_input))\n",
    "    for col in cat_cols:\n",
    "        X.append(np.array(test[col],dtype=np.int32).reshape(-1,1))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/kaggle/input/prepared-data-mlp-ffm/test_done.csv')\n",
    "extra = test[['id1','id2','id3','id5']]\n",
    "test = test[col_order]\n",
    "X_test = prepare_test(test, encoder, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra['pred_proba'] = y_pred\n",
    "extra.sort_values(by=['id2','pred_proba'],ascending=[True,False],inplace=True)\n",
    "extra.reset_index(drop=True,inplace=True)\n",
    "extra.drop(columns=['pred_proba'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra['pred'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra.to_csv('submission11.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:light"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7839013,
     "sourceId": 12428865,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
